# Rationale Dataset and Code Repository
This repository supports the research for the paper "Are LLM Rationales Ready to Give Feedback? Measuring the Reliability in Reading and Writing Tasks." It provides essential resources for the evaluation of rationale explanations generated by Large Language Models (LLMs) in both writing and reading tasks.

Contents:

Dataset: 
Includes 200 human-written texts assessed by LLMs for grammaticality, cohesiveness, likability, and relevance, along with 100 human-created reading comprehension questions. Each LLM-generated response includes a rationale explanation. All rationales were independently reviewed and validated by three human experts. (Available from Emily-Kim-NLP upon reasonable request)

Writing Evaluation Code:
Evaluates LLM-generated scores and supporting rationales for human-written texts using GPT-4o, GPT-3.5, Gemini-Pro, and Gemini-Flash. Metrics include accuracy, explanation validity, and inter-annotator agreement.

Reading Evaluation Code:
Assesses LLMsâ€™ responses and rationales for multiple-choice reading questions, focusing on the alignment between answers, supporting ideas, and explanation correctness.

This repository contributes to research on the feedback readiness and reliability of LLM-generated explanations in automated assessment settings. It provides tools and analysis frameworks to investigate how well LLM rationales align with human expectations for formative feedback in educational contexts.
